%\vspace{-0.1in}
\section{Background}
%\vspace{-0.1in}
Suppose we are given a $K$-mode tensor $\Ycal \in \mathbb{R}^{d_1 \times \ldots \times d_K}$. The $k$-th mode includes $d_k$ entities or nodes (\eg customers).  Each entry is indexed by a tuple $\bi = (i_1, \ldots, i_K)$ and stands for the interaction of the corresponding $K$ nodes.  The entry value is denoted by $y_{\bi}$. To decompose $\Ycal$, we  introduce $K$ latent factor matrices $\Ucal = \{\U^1, \ldots, \U^K\}$ to represent all the tensor nodes. Each $\U^k$ is $d_k \times r_k$, and its rows are the latent factors for the nodes in mode $k$. We aim to use $\Ucal$ to reconstruct the observed entries in $\Ycal$. A classical approach is Tucker decomposition~\citep{Tucker66}, which assumes $\Ycal = \Wcal \times_1 \U^{1} \times_2 \ldots \times_K \U^{K}$, where  $\mathcal{W} \in \mathbb{R}^{r_1 \times \ldots \times r_K}$ is a parametric tenor,  and $\times_k$ is the mode-$k$ tensor matrix product~\citep{kolda2006multilinear}, which resembles the ordinary matrix-matrix product.
When we set all $r_k = r$, and constrain $\Wcal$ to be diagonal, Tucker decomposition becomes  CANDECOMP/PARAFAC (CP) decomposition~\citep{Harshman70parafac}. While numerous tensor factorization methods have  been proposed, \eg  ~\citep{Chu09ptucker,kang2012gigatensor,choi2014dfacto}, most of them are inherently  based on the CP or Tucker decomposition form. However, since the both forms are mutilinear to the latent factors, they are incapable to capture more complicated, nonlinear relationships in data.

%To overcome this problem, recently a few nonparametric Bayesian decomposition methods were developed, \eg ~\citep{XuYQ12, zhe2016distributed}, which use Gaussian processes (GPs) to model the tensor entries as an unknown function of the corresponding latent factors, and can flexibly estimate various nonlinear relationships of the nodes. In many benchmark datasets, the nonparametric approaches significantly outperform the mutilinear methods in terms of the missing value prediction. 
On the other hand, real-world tensors are often supplemented with detailed temporal information, namely, the time stamps of the observed interactions.
%For example, from online shopping history, we can extract not only a three-mode \textit{(customer, commodity, online-store)} tensor, but also the time point at which each {customer} purchased a particular commodity from some store. 
To incorporate these information, traditional methods either  drop the time stamps  and perform count tensor decomposition~\citep{chi2012tensors, Hu2015CountTensor}, or  discretize the time stamps into discrete steps, \eg weeks or months,  augment the tensor with a time mode~\citep{xiong2010temporal, schein2015bayesian, Schein:2016:BPT:3045390.3045686}, and jointly estimate the time factors.  
Despite their simplicity and convenience, these methods overlook the rich temporal influences of the observed interactions on each other, and can miss valuable temporal patterns. To mitigate this issue, \citet{zhe2018stochastic} formulated \textit{even-tensor} to maintain all the accurate time stamps. In an event-tensor, each entry is an event sequence of a particular interaction, rather than a numerical value. \citet{zhe2018stochastic} modelled the observed entries as a set of mutually excited Hawkes processes~\citep{hawkes1971spectra}. The rate of the events in each entry  $\bi$ is%   $\blambda_\bi(t) = \blambda_\bi^0 + \sum\nolimits_{s_n<t} k(\x_{\bi_n},\x_{\bi})h_0(t-s_n) $, 
\begin{align}
\blambda_\bi(t) = \blambda_\bi^0 + \sum\nolimits_{s_n \in A(t)} k(\x_{\bi_n},\x_{\bi})h_0(t-s_n) \notag
\end{align}
where $\blambda_\bi^0$ is the background rate, $A(t)$ is a time window that specifies the range of dependent events happened before (\eg  $50$ events happened before and nearest to $t$), $\bi_n$ is the entry of the previous interaction at time $s_n$, $\x_{\bi_n}$ and $\x_{\bi}$ are the latent factors associated with the entry $\bi_n$ and $\bi$ respectively, $k(\cdot, \cdot)$ is a kernel function that measures their similarity, and $h_0(\cdot)$ is a base triggering kernel,  \eg $h_0(\Delta_t)=\beta\exp(-\frac{\Delta_t}{\tau})$. The model can estimates the (local) excitation effects of the previously happened interactions on the current one, and the triggering strength are (partly) encoded into the latent factors --- the closer the corresponding factors of the two entries, the stronger the strength. On both synthetic and real-world datasets, the model not only achieves better predictive performance, but also discovers interesting clusters with temporal meanings. 
 %To capture the nonlinear relationships of the nodes, the model further places a GP prior to sample  the background rate $\blambda_\bi^0$ as a (nonlinear) function of $\x_i$, the  factors associated with entry $\bi$.  On both synthetic and real-world datasets, the model not only achieves much better predictive performance, but also discovers interesting clusters with temporal meanings. 


%defects of the previous method, summary --> binary indicators, -->  rate --> the justfication --> GP --> joint distribution
\section{Model}
Although \citet{zhe2018stochastic}'s model can estimate various triggering effects among the interactions, it completely ignores another important class of temporal influence ---\textit{inhibition}, and hence may still fail to capture complex yet crucial temporal patterns. The mutual inhibition of the events  are ubiquitous in real world. For example, a customer who bought a Surface laptop is unlikely to buy a MacBook; people who just ate at a Chinese buffet restaurant are unlikely to order a sandwich at Burger King. In practice, among the interaction events can be mixed excitation and inhibition effects, resulting in complex temporal dependencies. In order to capture these dependencies to uncover the important structures and complex relationships, we propose an adaptive nonlinear event-tensor decomposition model that couples a general random point process with a latent Gaussian process (GP), presented as follows.

%First, following \citep{zhe2018stochastic}, we denote  the event sequence of each observed entry $\bi$ by $y_{\bi} = [s_\bi^1, \ldots, s_\bi^{n_\bi}]$, \ie the time stamps the interaction $\bi$ occurred, and $n_\bi$ is the number of occurrences. Given all the observed entries $\{y_\bi\}$, we can merge their event sequences into a single sequence $S = [(s_1, \bi_1). \ldots, (s_N, \bi_N)]$ where $s_1 \le \ldots \le s_N$, each $\bi_n$ is the entry index of the interaction 	

First, for each observed entry $\bi$, we denote its event sequence by $y_{\bi} = [s_\bi^1, \ldots, s_\bi^{n_\bi}]$, \ie the time stamps the interaction $\bi$ occurred, and $n_\bi$ is the number of occurrences. Note that each entry represents a particular type of interaction. We can merge the event sequences of all the observed entries into a single sequence, $S = [(s_1, \bi_1). \ldots, (s_N, \bi_N)]$, where $s_1 \le \ldots \le s_N$ are all the time stamps, each $\bi_n$  indexes the entry that event $s_n$ belongs to, \ie the particular interaction occurred at $s_n$. 

Now  we consider a probabilistic model to sample the observed entries in the event-tensor. In order to detect and disentangle the excitation and inhibition effects, we first sample a binary variable $z_{\bi\rightarrow \bj}$ for each pair of the entries $(\bi,\bj)$ to indicate the influence type --- $z_{\bi\rightarrow \bj} = 1$ means the occurrence of interaction $\bi$ excites  interaction $\bj$ to happen; otherwise $\bi$ suppresses the occurrence of $\bj$. Note that $\bi$ and $\bj$ can be identical. We use an uninformative prior distribution, $p(z_{\bi\rightarrow\bj}) \propto 1$.%, or   $p(z_{\bi\rightarrow\bj}=1) = p(z_{\bi\rightarrow\bj}=0) = \frac{1}{2}$. 

%point process, motivation 
Given the influence types between the interactions, $\Zcal = \{z_{\bi \rightarrow \bj}\}$, we construct a random point process to sample the observed entries (\ie event sequences), from which we want to encode both the excitation and inhibition effects into the latent factors. 
%One might consider to use the proximity (or similarity) between the factors to model these effects --- when the factors of two interactions (entries) are close, the triggering effect dominates; otherwise if their factors are far away, the suppressing effect takes control. Although simple and intuitive, this strategy is insufficient to model the strength of these effects. 
%For example, the factors being far away may also naturally represent a weak triggering strength between two interactions (the farther, the weaker; being too distant may lead to zero strength). However, it conflicts with the representation of the inhibition effects. Similar arguments apply to the modelling of inhibition strength. 
%For example, to exhibit a weak triggering strength between two interactions, it is natural to use distant factors for the associated nodes. However, this conflicts with the representation of the inhibition effect. Similar arguments apply to the inhibition strength. 
To this end, we introduce dual latent factors $\Ucal=\{\U^1,\ldots, \U^K\}$ and $\Vcal=\{\V^1, \ldots, \V^K\}$, one to encode the excitation effect and the other inhibition. The closeness between the factors in $\Ucal$  is used to express the excitation strength, while in  $\Vcal$ the inhibition strength.  
%specifically, 
%The dual factors will not only increase the model expressive power, but also discover the structures within the two opposite effects. 
%The closeness between the $\u_{j}^k$'s and between the $\v_j^k$'s reveal the triggering and suppressing strength, respectively. 
%The similarity between $\u_{j}^k$'s reveal the triggering strength, while the closeness between  $\v_{j}^k$'s the inhibition strength.  They are orthagonal and do not confliict with each other. 
%Our random point process is determined by the dual latent factors $\Ucal$ and $\Vcal$.
Specially, for each entry $\bi$, we first design a raw rate function that reflects both the triggering and suppressing influences from the previous interactions, 
\begin{align}
\tilde{\lambda}_\bi(t|\Zcal) = \lambda_\bi^0 + \sum_{s_n<t} \mathbbm{1}(z_{\bi_n\rightarrow \bi} = 1) k(\x_{\bi}, \x_{\bi_n}) h_0(t - s_n) - \mathbbm{1}(z_{\bi_n\rightarrow \bi} = 0) k(\hat{\x}_{\bi}, \hat{\x}_{\bi_n})g_0(t-s_n)\MoveEqLeft[1]\hspace{11pt}\raisetag{1.6\baselineskip}\label{eq:raw}%\notag
\end{align}
where $\blambda_\bi^0$ is the background rate, $\mathbbm{1}(\cdot)$ is the indicator function, $\x_{\bi_n}$ and $\x_{\bi}$ are the  factors of $\Ucal$ associated with  $\bi_n$ and $\bi$, $\x_{\bi} = [\U^1(i_1, :), \ldots, \U^K(i_K,:)]$ and $\x_{\bi_n} = [\U^1(i_{n_1}, :), \ldots, \U^K(i_{n_2},:)]$, 
 $\hat{\x}_{\bi}$ and $\hat{\x}_{\bi_n}$ are the factors of $\Vcal$ associated with $\bi_n$ and $\bi$, $\hat{\x}_{\bi} = [\V^1(i_1, :), \ldots, \V^K(i_K,:)]$ and $\hat{\x}_{\bi_n} = [\V^1(i_{n_1}, :), \ldots, \V^K(i_{n_2},:)]$,  $k(\cdot, \cdot)$ is a kernel function that measures the similarity of the input factors, and $h_0(\cdot), g_0(\cdot)$ are base triggering and suppressing functions that describe how the effect strength vary with time. We choose the commonly used exponential decaying function, $h_0(\Delta_t) = \beta_1\exp(-\frac{\Delta_t}{\tau_1})$ and $g_0(\Delta_t) = \beta_2\exp(-\frac{\Delta_t}{\tau_2})$.
 When $z_{\bi_n \rightarrow \bi} = 1$, the interaction $\bi_n$ happened at $s_n$ will increase the rate of $\bi$ and hence excite $\bi$ to happen at time $t$. The  strength is (partly) determined by the similarity of their associate latent factors in $\Ucal$, \ie $\x_{\bi}$ and $\x_{\bi_n}$.  When $z_{\bi_n \rightarrow \bi} = 0$, the occurrence of $\bi_n$ will decrease the rate, and hence suppress $\bi$ to happen at $t$. Similarly, the closer the associated factors in $\Vcal$, \ie $\tilde{\x}_{\bi}$ and $\tilde{\x}_{\bi_n}$, the stronger the inhibition strength. Hence, the excitation and inhibition effects are decoupled and encoded into the $\Ucal$ and $\Vcal$, respectively. This design further enables our model to discover the clustering structures within the two types of the effects. Specifically, the nodes in the same cluster/community (reflected in $\Ucal$ or $\Vcal$)  more strongly excite or inhibit each other to interact with the nodes of other modes from the same cluster, \eg ``buying the same brand of cosmetics'' and ``NOT eating the same type of food''. Note that the clusters in $\Ucal$ and $\Vcal$ are not necessarily the same. 
%,  Our model can discover the grouping structures hidden in the excitation and inhibition effects respectively. The nodes in same group/community more strongly excite or inhibit each other 
%Note that the clusters in $\Ucal$ and $\Vcal$ are not necessarily identical. 


%both the excitation and inhibition effects are encoded into the latent factors. 
%Such design enables our model to discover the grouping structures hidden in thetriggering effects — entities in the same group/community more strongly excite each other to interactwith other modes’ entities from the same group,e.g.,“purchasing the same brand of products" and“watching the same types of movies".

%The structures hidden in their strength are discovered and decoupled by the dual factors.  %need to say more about the structure
%As we can see from \eqref{eq:raw}, when $z_{\bi_n \rightarrow \bi} = 1$, the interaction $\bi_n$ happened at $s_n$ will increase the rate of $\bi$ and hence encourage $\bi$ to happen at time $t$. The triggering strength is partly determined by the similarity of their associate latent factors in $\Ucal$, \ie $\x_{\bi}$ and $\x_{\bi_n}$.  When $z_{\bi_n \rightarrow \bi} = 0$, the occurrence of $\bi_n$ will decrease the rate, and hence suppress $\bi$ to happen at $t$. Again, the closer the associated factors in $\Vcal$, \ie $\tilde{\x}_{\bi}$ and $\tilde{\x}_{\bi_n}$, the stronger the strength. In this way, both the excitation and inhibition effects are encoded into the latent factors. The structures hidden in their strength are discovered and decoupled by the dual factors.  %need to say more about the structure

Then to obtain a positive rate function so as to build a valid point process, we \cmt{follow~\citep{mei2017neural} to} apply a scaled soft-plus function over the raw rate $\tilde{\lambda}_\bi(t|\Zcal)$, 
\begin{align}
\lambda_\bi(t|\Zcal) = s_\bi \log\big(1 + \exp(\frac{\tilde{\lambda}_\bi(t|\Zcal)}{s_\bi})\big) \label{eq:rate}
\end{align}
where $s_\bi>0$. It is known that when $s_\bi \rightarrow 0$,  $\lambda_\bi  \rightarrow \max\big(\tilde{\lambda}_\bi, 0\big)$. Therefore the scaled soft-plus can considerably maintain the additive structure in our raw rate definition in \eqref{eq:raw}. While other transformation operators are also possible, \eg the exponential function, we found empirically that the scaled soft-plus exhibits superior and excellent performance.

Next, to capture the complex yet static relationships of the nodes, we consider in each entry $\bi$, the background rate $\lambda_\bi^0$ as a (possible) nonlinear function of the associated dual latent factors, $\x_{\bi}$ and  $\tilde{\x}_{\bi}$. To obtain a nonnegative background rate, we first sample a latent function $f(\x_{\bi}, \tilde{\x}_{\bi})$, and then set $\lambda_\bi^0 = \exp\big(f(\x_{\bi}, \tilde{\x}_{\bi})\big)$. To flexibly estimate $f(\cdot)$, we place a Gaussian process prior. Hence, the latent function values for all the observed entries are  sampled from a multivariate Gaussian distribution,
\begin{align}
		p(\f|\Ucal, \Vcal) = \N\big(\f|\0, c(\widehat{\X}, \widehat{\X})\big) \label{eq:gp}
\end{align} 
where each row of $\widehat{\X}$ are the dual latent factors for a particular entry, and $c(\cdot,\cdot)$ is the covariance (or kernel) function. %We can choose a nonlinear kernel function 

Finally, given the observed entry values $\{y_\bi\}$, \cmt{according to \eqref{eq:raw}\eqref{eq:rate}\eqref{eq:gp},} the joint probability of our model is 
\begin{align}
p(\Zcal, \{y_\bi, f_\bi\}|\Ucal, \Vcal) = \N\big(\f|\0, c(\widehat{\X}, \widehat{\X})\big)\prod_{\bi, \bj } p(z_{\bi\rightarrow \bj})\prod_{\bi}\exp(-\int_0^T \lambda_\bi(t|\Zcal)\d t) \prod_{n} \lambda_{\bi_n}(s_n|\Zcal) \label{eq:joint}
\end{align}
where $T$ is the total time span. Note that $\Zcal = \{\z_{\bi\rightarrow \bj}\}$, each base rate $\lambda_\bi^0$ in $\lambda_\bi(t|\Zcal)$ is $\exp(f_\bi)$, and all the entries $\{y_\bi\}$ can be  equivalently flatten into a single sequence $S = [(s_1, \bi_1), \ldots, (s_N, \bi_N)]$.

  
%discrete q(z), sparse GP & numerical stable scaled softplus
%challenages of inference 
%how to address
%numerical instablity 
\section{Algorithm}
Exact model inference is computationally infeasible when the observed entries are many. On one hand, since between every pair of entries $(\bi, \bj)$ we need to estimate the influence type $z_{\bi \rightarrow \bj}$,  the total number of the binary variables in $\Zcal$ can be tremendous. Due to the discrete nature, the estimation of $\Zcal$ may need a combinatorial search and can be prohibitively expensive. On the other hand, the GP term in \eqref{eq:joint} needs to compute the covariance matrix $c(\widehat{\X}, \widehat{\X})$ and its inverse, which interlaces all the factors $\{\Ucal, \Vcal\}$, and is not scalable to a large number of entries. To tackle these problems, we develop a  nested stochastic variational Expectation-Maximization(EM) algorithm, presented as follows. %we parameterize a relaxed, continuous distribution as the approximate posterior for each binary indicator,  combine with the sparse GP framework~\citep{titsias2009variational,hensman2013gaussian} and the reparameterization trick~\citep{kingma2013auto} to develop a nested stochastic variational Expectation-Maximization (EM) algorithm. We present our algorithm as follows. 

\subsection{Compact Concrete Binary Posterior}
First,  to avoid the combinatorial search in estimating $\Zcal$, we use a concrete binary distribution as the variational posterior of each $z_{\bi \rightarrow \bj}$,   
\begin{align}
q(z_{\bi \rightarrow \bj}|\pi_{\bi \rightarrow \bj}, \tau) = \frac{\tau\pi_{\bi \rightarrow \bj}(1 - \pi_{\bi \rightarrow \bj})z_{\bi \rightarrow \bj}^{-\tau-1}(1-z_{\bi \rightarrow \bj})^{-\tau-1} }{(\pi_{\bi \rightarrow \bj}z_{\bi \rightarrow \bj}^{-\tau} +(1 - \pi_{\bi \rightarrow \bj})(1-z_{\bi \rightarrow \bj})^{-\tau} )^2},  \label{eq:concrete}
\end{align}
where $z_{\bi \rightarrow \bj} \in [0,1]$,  $\pi_{\bi \rightarrow \bj}$ is a probability and $\tau$ a temperature hyperparameter.  This distribution is a continuous relaxation to the binomial distribution $\mathrm{Bio}(\pi_{\bi \rightarrow \bj})$. %that is  the ideal posterior form of $z_{\bi \rightarrow \bj}$. 
\cmt{The samples of $q$ are continuous and in the range $[0, 1]$.} The smaller the temperature  $\tau$, the closer $q$ is to the binomial distribution. When $\tau \rightarrow 0$, $q$ converges to $\mathrm{Bio}(\pi_{\bi \rightarrow \bj})$. 
 The major advantage of using $q$ (rather than $\mathrm{Bio}(\pi_{\bi \rightarrow \bj})$) as the variational posterior of $z_{\bi\rightarrow \bj}$ is that we can apply the reparameterization trick to generate continuous samples. These samples are parameterized with the probability $\pi_{\bi \rightarrow \bj}$, based on which, we can optimize $\pi_{\bi \rightarrow \bj}$ with efficient stochastic gradient ascent (the details will be given later). The concrete binary distribution is the special case of the Gumbel-Softmax distribution with two categories~\citep{jang2016categorical}. 
 
 %that are parameterized by the probability c$, with which to optimize xxx with stochastic gradient descent.  The concrete binary distribution is the special case of the Gumbel-Softmax distribution with only two categories [?]. 

However, if we consider all $\{\pi_{\bi \rightarrow \bj}\}$ as free parameters, we will have a parameter explosion even for a small number of entries (or interactions).  Consider we observed only $1000$ entries. We need to estimate $1000 \times 1000$ (\ie one million) parameters  to determine each $q(z_{\bi \rightarrow \bj})$. The huge number of parameters can dramatically  increase the  inference difficulty and computational cost. To overcome this problem, we further parameterize each $\pi_{\bi \rightarrow \bj}$ by the dual latent factors associated with $\bi$ and $\bj$, %$\pi_{\bi \rightarrow \bj} = \sigma\big(k(\x_{\bi}, \x_{\bj}) - k(\tilde{\x}_{\bi}, \tilde{\x}_{\bj})\big) $
\begin{align}
\pi_{\bi \rightarrow \bj} = \sigma\big(k(\x_{\bi}, \x_{\bj}) - k(\tilde{\x}_{\bi}, \tilde{\x}_{\bj})\big)  \label{eq:pi} 
\end{align}
where $\sigma(a) = 1/(1+ \exp(-a)$ is  the sigmoid function. If the similarity between the two entries' factors in $\Ucal$ that encode the excitation effect are larger than the similarity between their factors in $\Vcal$ encoding the inhibition effect, \ie $k(\x_{\bi}, \x_{\bj}) > k(\tilde{\x}_{\bi}, \tilde{\x}_\bj)$, the posterior probability $\pi_{\bi \rightarrow \bj} > 0.5$, meaning the occurrence of $\bi$ is more likely to trigger the occurrence of  $\bj$. Otherwise, $\pi_{\bi \rightarrow \bj} < 0.5$, $\bi$ is more probable to suppress $\bj$. Therefore, our parameterization of $\{\pi_{\bi \rightarrow \bj}\}$ not only greatly reduces the posterior parameters of $\Zcal$, but also guides the posterior inference toward an interpretable result. It will enforce the inference to capture the strength of the two kinds of temporal effects and transmit them into the corresponding latent factors. %In this way, we can better recover the hidden structures within the temporal interactions. 
	
\subsection{Nested Stochastic Variational Expectation Maximization}
Next, to enable scalable inference of the latent GP on the background rates (see \eqref{eq:gp}), we use the sparse variational GP framework~\citep{titsias2009variational,hensman2013gaussian}.  We first introduce $m$ inducing points $\B$ and targets $\b$,  and augment the GP likelihood in \eqref{eq:gp} with $p(\f, \b|\Ucal, \Vcal, \B) = \N([\f;\b]|\0, \C)$ where $\C = [c(\widehat{\X}, \widehat{\X}), c(\widehat{\X}, \B); c(\B, \widehat{\X}), c(\B, \B)]$ where $c(\widehat{\X}, \B)$ is the cross covariance between $\widehat{\X}$ and $\B$. Note that $m$ is much smaller than the number of entries in the data, and if we marginalize out $\b$, we recover $p(\f|\Ucal, \Vcal)$. We can then obtain an augmented model $p(\Zcal, \b, \{\y_\bi, f_{\bi}\}|\Ucal, \Vcal, \B)$ by substituting $\N([\f;\b]|\0, \C)$ for  $\N(\f|\0, c(\widehat{\X}, \widehat{\X}))$ in the joint probability \eqref{eq:joint}.

Following~\citep{hensman2013gaussian}, we define a variational posterior $q(\f,\b) = q(\b) p(\f|\b)$ where $q(\b) = \N(\b|\bmu, \bSigma)$ and $p(\f|\b)$ is the conditional Gaussian distribution obtained from their joint prior $p(\f, \b|\Ucal, \Vcal, \B)$. We then combine with $q(\Zcal) = \prod_{\bi, \bj} q(\z_{\bi\rightarrow \bj})$, and follow the standard framework to derive a decomposable variational lower bound of the model evidence, 
$\Lcal = \expt{q(\b)}{\log\frac{p(\b)}{q(\b)}} + \sum_{\bi, \bj} \EE_{q(\z_{\bi \rightarrow \bj})} \big[\log\frac{p(z_{\bi \rightarrow \bj})}{q(z_{\bi \rightarrow \bj})}\big]  -\sum_{\bi}\expt{q(\Zcal), q(f_\bi, \b)}{\int_{0}^T \lambda_\bi(t) \d t} + \sum_{n=1}^N \EE_{q(\Zcal), q(f_\bi, \b)} \big[\log\big(\lambda_{\bi_n}(s_n)\big)\big]$
where $q(f_\bi, b) = q(\b)p(f_\bi|\b)$. Here $p(f_\bi|\b) = \N(f_\bi|\eta_\bi,  \sigma^2_\bi)$
where $\eta_\bi = c([\x_\bi, \tilde{\x}_\bi], \B)c(\B,\B)^{-1}\b$ and $\sigma_\bi^2 = c([\x_\bi, \tilde{\x}_\bi], [\x_\bi, \tilde{\x}_\bi]) - c([\x_\bi, \tilde{\x}_\bi], \B)c(\B,\B)^{-1}c(\B, [\x_\bi, \tilde{\x}_\bi])$.

To handle both large numbers of tensor entries and interaction events, we  randomly partition the tensor entries, entry pairs and the observed events into mini-batches $\{O_l\}$, $\{Q_t\}$ and $\{N_k\}$, according to which we arrange our variational bound as the expectation of a stochastic bound, $\Lcal = \expt{p(t),p(l),p(k)}{\tilde{\Lcal}_{t,l,k}}$, where $p(t) = \frac{|Q_t|}{M^2}, p(l) = \frac{|O_l|}{M}, p(k) = \frac{|N_k|}{N}$, and 
\begin{align}
&\tilde{\Lcal}_{t,l,k} = \sum_{(\bi,\bj) \in Q_t}\frac{M^2}{|Q_t|} \expt{z_{\bi \rightarrow \bj}}{\log\frac{p(z_{\bi \rightarrow \bj})}{q(z_{\bi \rightarrow \bj})}} -\sum_{\bi \in O_l}\frac{M}{|O_l|}\expt{q(\Zcal), q(f_\bi, \b)}{\int_{0}^T \lambda_\bi(t) \d t} \notag\\
&+\sum_{n \in N_k}\frac{N}{|N_k|} \expt{q(\Zcal), q(f_\bi, \b)} {\log\big(\lambda_{\bi_n}(s_n)\big)} + \expt{q(\b)}{\log\frac{p(\b)}{q(\b)}}. \label{eq:st-elbo}
\end{align}
\cmt{
To handle both large numbers of tensor entries and interaction events, we develop a stochastic optimization algorithm for $\Lcal$. Specifically, we randomly partition the entries, entry pairs and the observed events into mini-batches $\{O_l\}$, $\{Q_t\}$ and $\{N_k\}$, according to  which we arrange the variational bound as 
\begin{align}
&\Lcal =  \sum_{t}\frac{|Q_t|}{M^2}\sum_{(\bi,\bj) \in Q_t}\frac{M^2}{|Q_t|} \expt{z_{\bi \rightarrow \bj}}{\log\frac{p(z_{\bi \rightarrow \bj})}{q(z_{\bi \rightarrow \bj})}} -\sum_{l}\frac{|O_l|}{M}\sum_{\bi \in O_l}\frac{M}{|O_l|}\expt{q(\Zcal), q(f_\bi, \b)}{\int_{0}^T \lambda_\bi(t) \d t} \notag \\
&+\sum_k\frac{|N_k|}{N}\sum_{n \in N_k}\frac{N}{|N_k|} \expt{q(\Zcal), q(f_\bi, \b)} {\log\big(\lambda_{\bi_n}(s_n)\big)} + \expt{q(\b)}{\log\frac{p(\b)}{q(\b)}}. 
\end{align} 
The bound can therefore be considered as the expectation of a stochastic bound, $\Lcal = \expt{p(t),p(l),p(k)}{\tilde{\Lcal}_{t,l,k}}$, where $p(t) = \frac{|Q_t|}{M^2}, p(l) = \frac{|O_l|}{M}, p(k) = \frac{|N_k|}{N}$, and 
\begin{align}
&\tilde{\Lcal}_{t,l,k} = \sum_{(\bi,\bj) \in Q_t}\frac{M^2}{|Q_t|} \expt{z_{\bi \rightarrow \bj}}{\log\frac{p(z_{\bi \rightarrow \bj})}{q(z_{\bi \rightarrow \bj})}} -\sum_{\bi \in O_l}\frac{M}{|O_l|}\expt{q(\Zcal), q(f_\bi, \b)}{\int_{0}^T \lambda_\bi(t) \d t} \notag\\
&+\sum_{n \in N_k}\frac{N}{|N_k|} \expt{q(\Zcal), q(f_\bi, \b)} {\log\big(\lambda_{\bi_n}(s_n)\big)} + \expt{q(\b)}{\log\frac{p(\b)}{q(\b)}}.
\end{align}
}
We can therefore maximize $\Lcal$ with stochastic optimization. Each time, we sample three mini-batches, $Q_t$, $O_l$ and $N_k$ for tensor entries, entry pairs and the interaction events respectively, and optimize the stochastic bound $\tilde{\Lcal}_{t,l,k}$ to update the latent factors and the variational posteriors. 

%Given the stochastic bound $\tilde{\Lcal}_{t,l,k}$,
 However, there remains a hurdle ---  all the expectation terms except $\expt{q(\b)}{\log\frac{p(\b)}{q(\b)}}$ in the stochastic bound (see \eqref{eq:st-elbo}) are not analytical due to the complicated integrands (see the definition of $q(z_{\bi \rightarrow \bj})$ in \eqref{eq:concrete} and $\lambda_\bi(t)$ in \eqref{eq:rate}).  Hence we cannot  compute the gradient of $\tilde{\Lcal}_{t,l,k}$ for optimization. To address this issue, we further use the reparameterization trick to compute a stochastic gradient of the stochastic variational bound  $\tilde{\Lcal}_{t,l.k}$, based on which we jointly update the variational posteriors $\{q(\Zcal), q(\b)\}$, the dual factors $\{\Ucal. \Vcal\}$, the inducing points $\B$ and other parameters such as the kernel parameters. We refer to our approach as a nested stochastic variational Expectation-Maximization (EM) algorithm, where the E and M steps are performed jointly. Specifically, for each entry pair $(\bi, \bj)$ in the stochastic bound, we sample a noise variable $\epsilon_{\bi\rightarrow \bj}$ from the standard logistic distribution, $p(\epsilon_{\bi\rightarrow \bj} ) = {\exp(-\epsilon_{\bi\rightarrow \bj})}/{(1 + \exp(-\epsilon_{\bi\rightarrow \bj}))^2}$. Then, we construct a parameterize sample, $z^*_{\bi\rightarrow \bj} = {1}/\big[{1 + \exp\big(-\frac{1}{\tau}(\log(\pi_{\bi \rightarrow \bj}) - \log(1 - \pi_{\bi \rightarrow \bj}) + \epsilon_{\bi\rightarrow \bj})\big)}\big].$
%\[
 %z^*_{\bi\rightarrow \bj} = \frac{1}{1 + \exp\big(-\frac{1}{\tau}(\log(\pi_{\bi \rightarrow \bj}) - \log(1 - \pi_{\bi \rightarrow \bj}) + \epsilon_{\bi\rightarrow \bj})\big)}.
 %\]
 It can be shown that $z^*_{\bi \rightarrow \bj}$ is a sample of $q(z_{\bi \rightarrow \bj})$. Similarly, for each entry $\bi$ in $\tilde{\Lcal}_{t,l,k}$, we generate parameterized samples $\{f_\bi^*, \b^*\}$ from $q(f_\bi, \b)$. Specifically, we sample a standard Gaussian noise $\bxi$, and set $\b^* = \bmu + \L \bxi$. To generate $f_\bi^*$, we first derive the marginal posterior $q(f_\bi) = \N(f_\bi|\hat{\eta}_\bi, \hat{\sigma}_\bi^2)$, where $\hat{\eta}_\bi = c([\x_\bi, \tilde{\x}_\bi], \B)c(\B,\B)^{-1}\bmu$ and $\hat{\sigma}_\bi^2 = \sigma_\bi^2 + c([\x_\bi, \tilde{\x}_\bi], \B)c(\B,\B)^{-1}\bSigma c(\B,\B)^{-1} c(\B, [\x_\bi, \tilde{\x}_\bi])$. Then we sample a noise $\xi$ from $\N(\xi|0, 1)$ and set $f_\bi^*= \hat{\eta}_\bi + \hat{\sigma}_\bi \xi$. Now we remove all the the expectation operations in $\tilde{\Lcal}_{t,l,k}$, and replace the latent random variables by their parameterized samples. Then we obtain an unbiased estimation of $\tilde{\Lcal}_{t,l,k}$, which we denote  by $\tilde{\Lcal}_{t,l,k}^*$. Note that $\EE(\tilde{\Lcal}_{t,l,k}^*) = \tilde{\Lcal}_{t,l,k}$ and $\EE(\tilde{\Lcal}_{t,l,k}) = \Lcal$. 
 We then compute the gradient of $\tilde{\Lcal}_{t,l,k}^*$, and update all the parameters $\btheta$ by $\btheta \leftarrow \btheta + \eta \frac{\partial \tilde{\Lcal}_{t,l,k}^*}{\partial \btheta}$, where $\eta$ is the learning rate and can be adjusted dynamically. We repeatedly sample the mini-batches, generate parameterized samples, and compute the stochastic gradient for updates, until convergence or the maximum number of iterations have been finished. 
 
 %We implement our algorithm with TensorFlow, which can automatically finish the tedious gradient computation. 
 

\subsection{Numerical Stability of Log Scaled Soft-Plus}
While the scaled soft-plus function can maintain the additive property of our rate function (see \eqref{eq:raw}\eqref{eq:rate}) and empirically shows excellent predictive performance, it can incur numerical issues in the inference procedure. The reason is that we need to compute the logarithm of the rate in each observed entry (see \eqref{eq:elbo}), $\log(\lambda_\bi) = \log(s_\bi) + \log \log (1 + \exp(\frac{\tilde{\lambda}_\bi}{s_\bi})).$
%\[
%\log(\lambda_\bi) = \log(s_\bi) + \log \log (1 + \exp(\frac{\tilde{\lambda}_\bi}{s_\bi})).
%\]
%In our inference, we jointly estimate  $\s_\bi$ with the other [paramters].  
When the raw rate $\tilde{\lambda}_\bi <0$ and $s_\bi$ is close to $0$, say, $0.01$,  their ratio $\frac{\tilde{\lambda}_\bi}{s_\bi} \ll 0$ may cause an underflow of the exponential term $\exp(\frac{\tilde{\lambda}_\bi}{s_\bi})$. As a consequence, $\exp(\frac{\tilde{\lambda}_\bi}{s_\bi})$ can be directly truncated to $0$, leading to  $\log (1 + \exp(\frac{\tilde{\lambda}_\bi}{s_\bi}))=0$. When the logarithm is taken again, the value becomes $-\infty$ and incurs the numerical issues. To overcome this problem, we define $x=\exp(\frac{\tilde{\lambda}_\bi}{s_\bi})$, and expand $\log(1+x)$ at $x = 0$ (Taylor expansion), and obtain  $\log(1+x) = x - \frac{1}{2}x^2 + o(x^3)$. Obvious, when $x\approx 0$ (\ie $\frac{\tilde{\lambda}_\bi}{s_\bi} \ll 0$), $\log(1+x) \approx x - \frac{1}{2}x^2$. Then, we can derive that $\log\log (1+x) = \log( x - \frac{1}{2}x^2 + o(x^3)) = \log(x) + \log(1 - \frac{1}{2}x + o(x^2))$. Expand the second term again, we can obtain $\log\log(1+x) = \log(x) - \frac{1}{2}x + o(x^2)$. Therefore, substituting back $\exp(\frac{\tilde{\lambda}_\bi}{s_\bi})$ for $x$, we obtain that when $\exp(\frac{\tilde{\lambda}_\bi}{s_\bi}) \ll 0$,
\begin{align}
\log(\lambda_\bi) \approx \log(s_\bi) + \frac{\tilde{\lambda}_\bi}{s_\bi} - \exp(\frac{1}{2}\frac{\tilde{\lambda}_\bi}{s_\bi}). \label{eq:sf}
\end{align}
Our inference algorithm jointly estimates $s_\bi$ with the other parameters. To maintain the numerical stability, we test the value  of $\exp(\frac{\tilde{\lambda}_\bi}{s_\bi})$ before we compute each $\log(\lambda_\bi)$. If it is less than a threshold (we took $-10$ in our experiments), we switch to the approximation in \eqref{eq:sf}, which ensures safe and reliable computational results. 
\subsection{Algorithm Complexity}
The time complexity of our algorithm is $\Ocal(m^3 + (m^2 + n_on_t + n_e) N)$, where $m$ is the number of inducing points, $n_o$ and $n_e$ are the sizes of the mini-batch for the observed entries and events, respectively, $n_t$ is the number of sampled time points. Since $m, n_o, n_t, n_e \ll N$, the time complexity is linear to the number of events. The space complexity is $\Ocal(\sum_{k=1}^K 2r_k d_k + m^2)$, which is to store the dual latent factors, the covariance matrix of $\f$ and the other parameters. 
