\vspace{-0.1in}
\section{Related Work}
\vspace{-0.1in}
Classical tensor factorization methods  include CP~\citep{Harshman70parafac}  and Tucker~\citep{Tucker66} decompositions, which are multilinear. Many other approaches have been proposed based on them ~\citep{ShashuaH05,Chu09ptucker,sutskever2009modelling,acar2011scalable,hoff_2011_csda,kang2012gigatensor,YangDunson13Tensor,RaiDunson2014,choi2014dfacto,hu2015zero,raiscalable}, to name a  few. Recently, several Bayesian nonparametric factorization models ~\citep{XuYQ12,zhe2015scalable,zhe2016distributed} were proposed to estimate the nonlinear relationships in data. To handle temporal information, current methods  either factorize the counts instead~\citep{chi2012tensors,HaPlKo15,Hu2015CountTensor}, or  discretize the time stamps into crude steps and then factorize the counts in each step~\citep{xiong2010temporal, schein2015bayesian,Schein:2016:BPT:3045390.3045686}.
Despite their success, these methods ignore  the rich temporal dependencies between the interactions, and can miss important temporal patterns. Recently, \citet{zhe2018stochastic} used the Hawkes processes to capture various, fine-grained triggering effects among the interactions. The method outperforms the previous approaches in terms of prediction, and discovers interesting clusters with temporal meanings. However, this method ignores another important temporal effect, inhibition, hence can still fail to capture complex, mixed temporal dependency patterns. In addition, the method sets a local time window to enable efficient computation, but meanwhile misses the valuable, long-range influences of the events on each other. To overcome these problems, we use latent factors to construct more flexible random point processes to detect and disentangle a variety of mixed excitation and inhibition effects, encoding them into the latent factors. In addition, we develop an efficient inference algorithm that is able to capture all kinds of short-term and long-term dependencies. 

Hawkes process (HP) is a popular class of random point processes to study mutual excitations within various events. Many models use HPs to discover temporal relationships, \eg~\citep{blundell2012modelling,tan2016content,linderman2014discovering,du2015dirichlet,he2015hawkestopic,wang2017predicting}. Several pieces of work were also proposed to improve the learning of  HPs, such as nonparametric triggering kernel estimation~\citep{zhou2013learning}, Granger causality~\citep{xu2016learning},  short doubly-censored event sequences~\citep{xu2017learning} and online estimation~\citep{yang2017online}.  Recently, \citet{mei2017neural} proposed neural Hawkes processes, which uses a continuous LSTM to  model complex temporal dependencies among the events (\eg triggering, suppression and their nonlinear interactions). They also tested to use the softplus function to integrate different temporal effects. Distinct from their excellent work, our method (1) uses the latent factors (rather than free parameters) to construct the point process to encode the temporal effects into the factor representations, and (2) investigates the properties of the rate function to fulfill efficient inference, especially for large numbers of events and event types.
